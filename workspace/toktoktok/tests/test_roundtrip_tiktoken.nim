import std/unittest
import std/os

import workspace/toktoktok

const TOKENIZERS_DIR = currentSourcePath().parentDir() / "tokenizers"

proc runTiktokenizerTests() =
  suite "Tiktokenizer Tests":

    test "load tiktokenizer file not found":
      expect TokenizerError:
        discard loadTiktokenizer("nonexistent.tiktoken", Gpt2Regexp)

    const TiktokenPairs = [
      ("r50k_base", "r50k_base.tiktoken", R50kRegexp),
      ("p50k_base", "p50k_base.tiktoken", P50kRegexp),
      ("cl100k_base", "cl100k_base.tiktoken", Cl100kRegexp),
      ("o200k_base", "o200k_base.tiktoken", O200kRegexp),
      ("kimik2.5", "kimik2.5.tiktoken", KimiK25Regexp),
    ]

    for tokenizerPair in TiktokenPairs:
      let (name, filename, regexp) = tokenizerPair
      let path = TOKENIZERS_DIR / filename

      test "load and decode (" & name & ")":
        doAssert fileExists(path), name & " tiktokenizer not found: " & path
        let tokenizer = loadTiktokenizer(path, regexp)
        let encoded = tokenizer.encode("Hello, world!")
        check encoded.len > 0

        let decoded = decodeToString(tokenizer, encoded)
        check decoded.len >= 5 and decoded[0..4] == "Hello"

      test "byte encoding roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let text = "Hello, world!"
        let encoded = tokenizer.encode(text)
        let decodedStr = decodeToString(tokenizer, encoded)
        check decodedStr == text

      test "CJK roundtrip - Chinese (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "ä½ å¥½ä¸–ç•Œ"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "CJK roundtrip - Japanese (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "ã“ã‚“ã«ã¡ã¯"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "CJK roundtrip - Korean (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Russian roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Hebrew roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "×©×œ×•× ×¢×•×œ×"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Khmer roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "áŸá½áŸáŸ’áá¸á–á·á—á–á›áŸ„á€"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Emoji roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "Hello ğŸŒ World! ğŸ‰"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Mixed CJK and English roundtrip (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "Hello ä¸–ç•Œ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Chinese historical paragraph issue merging 'ã€‚\\n' (" & name & ")":
        let tokenizer = loadTiktokenizer(path, regexp)
        let original = "ç´…ã€‚ç™½\né«®æ¼æ¨µæ±Ÿæ¸šä¸Šï¼Œæ…£çœ‹ç§‹æœˆæ˜¥é¢¨ã€‚ä¸€å£ºæ¿é…’å–œç›¸é€¢ï¼šå¤ä»Šå¤šå°‘äº‹ï¼Œéƒ½ä»˜ç¬‘è«‡ä¸­ã€‚\n\nã€€ã€€è©±èªªå¤©ä¸‹å¤§å‹¢ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ï¼šå‘¨æœ«ä¸ƒåœ‹åˆ†çˆ­ï¼Œå¹¶å…¥æ–¼ç§¦ã€‚åŠç§¦æ»…ä¹‹å¾Œï¼Œæ¥š\nã€æ¼¢åˆ†çˆ­ï¼Œåˆå¹¶å…¥æ–¼æ¼¢ã€‚æ¼¢æœè‡ªé«˜ç¥–æ–¬ç™½è›‡è€Œèµ·ç¾©ï¼Œä¸€çµ±å¤©ä¸‹ã€‚å¾Œä¾†å…‰æ­¦ä¸­èˆˆï¼Œå‚³è‡³ç»\nå¸ï¼Œé‚åˆ†ç‚ºä¸‰åœ‹ã€‚æ¨å…¶è‡´äº‚ä¹‹ç”±ï¼Œæ®†å§‹æ–¼æ¡“ã€éˆäºŒå¸ã€‚æ¡“å¸ç¦éŒ®å–„é¡ï¼Œå´‡ä¿¡å®¦å®˜ã€‚åŠæ¡“\nå¸å´©ï¼Œéˆå¸å³ä½ï¼Œå¤§å°‡è»ç«‡æ­¦ã€å¤ªå‚…é™³è•ƒï¼Œå…±ç›¸è¼”ä½ã€‚æ™‚"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

when isMainModule:
  runTiktokenizerTests()
