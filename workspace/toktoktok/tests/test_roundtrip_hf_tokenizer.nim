import std/unittest
import std/os

import workspace/toktoktok

const TOKENIZERS_DIR = currentSourcePath().parentDir() / "tokenizers"

proc runHfTokenizerTests*() =
  suite "HF Tokenizer Tests":



    test "load tokenizer file not found":
      expect TokenizerError:
        discard loadHFTokenizer("nonexistent.json")

    const TokenizerPairs = [
      ("gpt2", "gpt2-tokenizer.json"),
      ("llama3", "llama3-tokenizer.json"),
      ("minimax-m2.1", "minimax-m2.1-tokenizer.json"),
      ("glm-4.7", "glm-4.7-tokenizer.json"),
      ("exaone", "exaone-tokenizer.json"),
      ("step-3.5-flash", "step-3.5-flash-tokenizer.json"),
    ]

    for tokenizerPair in TokenizerPairs:
      let (name, filename) = tokenizerPair
      let path = TOKENIZERS_DIR / filename

      test "load and decode (" & name & ")":
        doAssert fileExists(path), name & " tokenizer not found: " & path
        let tokenizer = loadHFTokenizer(path)
        let encoded = tokenizer.encode("Hello, world!")
        check encoded.len > 0

        let decoded = decodeToString(tokenizer, encoded)
        check decoded.len >= 5 and decoded[0..4] == "Hello"

      test "byte encoding roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let text = "Hello, world!"
        let encoded = tokenizer.encode(text)
        let decodedStr = decodeToString(tokenizer, encoded)
        check decodedStr == text

      test "CJK roundtrip - Chinese (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "ä½ å¥½ä¸–ç•Œ"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "CJK roundtrip - Japanese (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "ã“ã‚“ã«ã¡ã¯"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "CJK roundtrip - Korean (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Russian roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Hebrew roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "×©×œ×•× ×¢×•×œ×"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Khmer roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "áŸá½áŸáŸ’áá¸á–á·á—á–á›áŸ„á€"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Emoji roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "Hello ğŸŒ World! ğŸ‰"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Mixed CJK and English roundtrip (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "Hello ä¸–ç•Œ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Chinese historical paragraph regression (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "ç´…ã€‚ç™½\né«®æ¼æ¨µæ±Ÿæ¸šä¸Šï¼Œæ…£çœ‹ç§‹æœˆæ˜¥é¢¨ã€‚ä¸€å£ºæ¿é…’å–œç›¸é€¢ï¼šå¤ä»Šå¤šå°‘äº‹ï¼Œéƒ½ä»˜ç¬‘è«‡ä¸­ã€‚\n\nã€€ã€€è©±èªªå¤©ä¸‹å¤§å‹¢ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ï¼šå‘¨æœ«ä¸ƒåœ‹åˆ†çˆ­ï¼Œå¹¶å…¥æ–¼ç§¦ã€‚åŠç§¦æ»…ä¹‹å¾Œï¼Œæ¥š\nã€æ¼¢åˆ†çˆ­ï¼Œåˆå¹¶å…¥æ–¼æ¼¢ã€‚æ¼¢æœè‡ªé«˜ç¥–æ–¬ç™½è›‡è€Œèµ·ç¾©ï¼Œä¸€çµ±å¤©ä¸‹ã€‚å¾Œä¾†å…‰æ­¦ä¸­èˆˆï¼Œå‚³è‡³ç»\nå¸é‚åˆ†ç‚ºä¸‰åœ‹ã€‚æ¨å…¶è‡´äº‚ä¹‹ç”±ï¼Œæ®†å§‹æ–¼æ¡“ã€éˆäºŒå¸ã€‚æ¡“å¸ç¦éŒ®å–„é¡ï¼Œå´‡ä¿¡å®¦å®˜ã€‚åŠæ¡“\nå¸å´©ï¼Œéˆå¸å³ä½ï¼Œå¤§å°‡è»ç«‡æ­¦"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

      test "Jules Verne passage with runic characters (" & name & ")":
        let tokenizer = loadHFTokenizer(path)
        let original = "En voici le fac-similÃ© exact.  Je tiens Ã  faire connaÃ®tre ces signes bizarres, car ils amenÃ¨rent le professeur Lidenbrock et son neveu Ã  entreprendre la plus Ã©trange expÃ©dition du dix-neuviÃ¨me siÃ¨cle:\n\n    á›¯  . á›¦ áš³ á›š á›š áš¼    á›… áš¼ á›¦ á›… áš¢ á›… á›š    áš¼ á›… á›… áš´ á› áš¦ á›…\n    áš¼ á› á› áš¼ áš¼ á›˜ áš     áš¢ áš³ á› á›… á› á›… áš     áš³ á› á›… áš¦ á›¦ áš´ á›…\n    áš´ á›  , áš¼ á› á›˜ áš³    á› á› á›¦ á› á› á›…_áš¼_  _áš¼_á› áš­ áš¦ á›¦ á›¦ áš³\n    á›… á›˜ á› áš³ á› á›…_á›_   áš³ áš¢ á› á›… áš´ á›       á›¦ á›¦ á› á›š_áš¼_á›\n   _á›_á› áš¢ á› á› á›¦        . áš³ áš¼ áš´ á›¦ áš´       á› á›… á› á› áš² áš¼\n    áš´ áš´ áš¦ á›¦ á›˜ á›       á›… á›… áš¢ á› áš¢ á›š       áš  á›¦ á› áš³ á› áš¢\n    áš¦ á›  , á› á› áš´       áš­ áš¼ á›… á› áš² áš­      _áš´_á›… áš¦ á› á›_á›¦_"
        let encoded = tokenizer.encode(original)
        let decoded = decodeToString(tokenizer, encoded)
        check decoded == original

when isMainModule:
  runHfTokenizerTests()
