[
  {
    "name": "chinese",
    "text": "‰Ω†Â•Ω‰∏ñÁïå",
    "token_ids": [
      56658,
      4083
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "japanese",
    "text": "„Åì„Çì„Å´„Å°„ÅØ",
    "token_ids": [
      36334
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "korean",
    "text": "ÏïàÎÖïÌïòÏÑ∏Ïöî ÏÑ∏Í≥Ñ",
    "token_ids": [
      11878,
      175354,
      36372
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "russian",
    "text": "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "token_ids": [
      45775,
      31016,
      158440
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "hebrew",
    "text": "◊©◊ú◊ï◊ù ◊¢◊ï◊ú◊ù",
    "token_ids": [
      63569,
      44776,
      33154,
      157,
      121556,
      33154,
      156,
      104990
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "khmer",
    "text": "·ûü·ûΩ·ûü·üí·ûè·û∏·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ",
    "token_ids": [
      76300,
      159,
      76300,
      189,
      76300,
      159,
      157246,
      146,
      76300,
      143,
      76300,
      184,
      76300,
      150,
      76300,
      183,
      76300,
      151,
      76300,
      150,
      76300,
      155,
      157246,
      132,
      76300,
      128
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "emoji",
    "text": "Hello üåç World! üéâ",
    "token_ids": [
      19739,
      9753,
      149144,
      5476,
      33,
      171182,
      137
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "mixed_cjk",
    "text": "Hello ‰∏ñÁïå „Åì„Çì„Å´„Å°„ÅØ ÏïàÎÖï",
    "token_ids": [
      19739,
      112507,
      32,
      36334,
      15194,
      124558
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "hello_world",
    "text": "Hello, world!",
    "token_ids": [
      19739,
      44,
      2035,
      33
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "greek_basic",
    "text": "ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Œ∫Œ±Œπ ŒºŒ±Œ∏Œ∑ŒºŒ±œÑŒπŒ∫Œ¨",
    "token_ids": [
      126137,
      131433,
      128593,
      90996,
      43292,
      122987,
      25008,
      11780,
      44497,
      33128,
      112062,
      20669,
      90996,
      43292
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "greek_alphabet",
    "text": "Œë Œ± Œí Œ≤ Œì Œ≥ Œî Œ¥ Œï Œµ Œñ Œ∂ Œó Œ∑ Œò Œ∏ Œô Œπ Œö Œ∫ Œõ Œª Œú Œº Œù ŒΩ Œû Œæ Œü Œø Œ† œÄ Œ° œÅ Œ£ œÉ Œ§ œÑ Œ• œÖ Œ¶ œÜ Œß œá Œ® œà Œ© œâ",
    "token_ids": [
      98059,
      27562,
      6437,
      146,
      40385,
      101684,
      61543,
      57855,
      49007,
      181857,
      53726,
      6437,
      150,
      6437,
      182,
      6437,
      151,
      130353,
      6437,
      152,
      64461,
      6437,
      153,
      6437,
      185,
      174932,
      45019,
      6437,
      155,
      63233,
      6437,
      156,
      25008,
      6437,
      157,
      89868,
      6437,
      158,
      6437,
      190,
      6437,
      159,
      99767,
      148537,
      41005,
      6437,
      161,
      126960,
      119224,
      46983,
      193056,
      32345,
      6437,
      165,
      13726,
      133,
      178559,
      90799,
      6437,
      167,
      121592,
      6437,
      168,
      196763,
      153434,
      125358
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_endo",
    "text": "Check the Endomorphism for p mod 3 == 1",
    "token_ids": [
      10309,
      275,
      1210,
      6004,
      94591,
      360,
      273,
      1057,
      32,
      51,
      2044,
      32,
      49
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_cube_roots",
    "text": "x¬≥‚àí1=0 <=> (x‚àí1)(x¬≤+x+1) = 0, if x != 1, x solves (x¬≤+x+1) = 0 <=> x = (-1¬±‚àö3)/2",
    "token_ids": [
      120,
      41672,
      14919,
      49,
      61,
      48,
      11161,
      62,
      359,
      120,
      14919,
      49,
      6536,
      120,
      21038,
      55114,
      43,
      49,
      41,
      409,
      32,
      48,
      44,
      730,
      1905,
      4072,
      32,
      49,
      44,
      1905,
      80427,
      359,
      120,
      21038,
      55114,
      43,
      49,
      41,
      409,
      32,
      48,
      11161,
      62,
      1905,
      409,
      17003,
      49,
      27680,
      43128,
      51,
      17093,
      50
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_curve",
    "text": "y¬≤ = x¬≥ + b, and y¬≤ = (xùúë)¬≥ + b <=> y¬≤ = x¬≥ + b (with ùúë¬≥ == 1) so we are still on the curve",
    "token_ids": [
      121,
      21038,
      409,
      1905,
      41672,
      1349,
      287,
      44,
      306,
      330,
      21038,
      409,
      359,
      120,
      20968,
      156,
      145,
      41,
      41672,
      1349,
      287,
      11161,
      62,
      330,
      21038,
      409,
      1905,
      41672,
      1349,
      287,
      359,
      4953,
      32,
      20968,
      156,
      145,
      41672,
      2044,
      32,
      49,
      41,
      612,
      563,
      457,
      2235,
      375,
      275,
      15450
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_lambda",
    "text": "Œª·µ©¬≤ + Œª·µ© + 1 ‚â° 0 (mod r) and ùúë¬≤ + ùúë + 1 ‚â° 0 (mod p)",
    "token_ids": [
      26046,
      134820,
      169,
      21038,
      1349,
      63233,
      134820,
      169,
      1349,
      32,
      49,
      159495,
      32,
      48,
      359,
      4898,
      354,
      41,
      306,
      32,
      20968,
      156,
      145,
      21038,
      1349,
      32,
      20968,
      156,
      145,
      1349,
      32,
      49,
      159495,
      32,
      48,
      359,
      4898,
      273,
      41
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_scalar",
    "text": "[a]P to represent P+P+ .... + P",
    "token_ids": [
      65323,
      93,
      80,
      301,
      3137,
      371,
      114010,
      43,
      51262,
      1349,
      371
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_bilinear",
    "text": "e: ùîæ1 x ùîæ2 -> ùîæt that map is bilinear e([a]P, [b]Q) = e(P, Q)·µÉ·µá",
    "token_ids": [
      101,
      58,
      32,
      20968,
      148,
      190,
      49,
      1905,
      32,
      20968,
      148,
      190,
      50,
      3736,
      32,
      20968,
      148,
      190,
      116,
      389,
      5648,
      355,
      179816,
      316,
      9277,
      97,
      93,
      80,
      44,
      791,
      98,
      93,
      81,
      41,
      409,
      316,
      15284,
      44,
      1696,
      41,
      134820,
      131,
      134820,
      135
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_srs",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau",
    "token_ids": [
      115,
      11686,
      11199,
      49,
      58,
      3724,
      49,
      93,
      73173,
      44,
      791,
      20669,
      93,
      73173,
      44,
      791,
      20669,
      21038,
      93,
      73173,
      44,
      4831,
      791,
      20669,
      39379,
      191,
      39379,
      187,
      83184,
      93,
      73173,
      93,
      964,
      3431,
      17370,
      300,
      53562
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_commitment",
    "text": "commit(srs_g1, blob) -> commitment C = ‚àë blob·µ¢.srs_g1·µ¢ = ‚àë [blob·µ¢.œÑ‚Å±]‚ÇÅ = [p(œÑ)]‚ÇÅ",
    "token_ids": [
      29566,
      3389,
      11686,
      11199,
      49,
      44,
      66949,
      41,
      3736,
      15095,
      347,
      409,
      15365,
      145,
      66949,
      134820,
      162,
      1411,
      11686,
      11199,
      49,
      134820,
      162,
      409,
      15365,
      145,
      791,
      28430,
      134820,
      162,
      46,
      20669,
      39379,
      177,
      93,
      73173,
      409,
      791,
      112,
      40,
      20669,
      18307,
      73173
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_witness",
    "text": "w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      119,
      4704,
      44,
      1443,
      41,
      409,
      359,
      112,
      4704,
      41,
      661,
      273,
      21290,
      2261,
      1180,
      359,
      120,
      27658,
      41
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_verification",
    "text": "e([proof]‚ÇÅ, [œÑ]‚ÇÇ - [z]‚ÇÇ) = e(C - [y]‚ÇÅ, [1]‚ÇÇ)",
    "token_ids": [
      101,
      9277,
      18234,
      93,
      73173,
      44,
      791,
      20669,
      93,
      67671,
      661,
      791,
      122,
      93,
      67671,
      41,
      409,
      316,
      11193,
      661,
      791,
      121,
      93,
      73173,
      44,
      791,
      49,
      93,
      67671,
      41
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "ecc_lagrange",
    "text": "œâ ‚àà ùîΩr a root of unity of order n, i.e. œâ‚Åø = 1",
    "token_ids": [
      35921,
      96957,
      32,
      20968,
      148,
      189,
      114,
      258,
      7724,
      300,
      32924,
      300,
      2417,
      311,
      44,
      1312,
      5318,
      46,
      125358,
      39379,
      191,
      409,
      32,
      49
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "group_basics",
    "text": "A group is a set of elements with a binary operation called the group law with a neutral element and an inverse.",
    "token_ids": [
      65,
      2746,
      355,
      258,
      1157,
      300,
      6387,
      418,
      258,
      14974,
      7311,
      3431,
      275,
      2746,
      2749,
      418,
      258,
      17497,
      4871,
      306,
      292,
      31633,
      46
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "field_basics",
    "text": "A field is a set of elements with two group laws, addition and multiplication, with corresponding inverse properties.",
    "token_ids": [
      65,
      2921,
      355,
      258,
      1157,
      300,
      6387,
      418,
      1451,
      2746,
      8349,
      44,
      4554,
      306,
      45271,
      44,
      418,
      9699,
      31633,
      7107,
      46
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "finite_field",
    "text": "ùîΩr is a finite-field of prime order r",
    "token_ids": [
      20968,
      148,
      189,
      114,
      355,
      258,
      20109,
      45471,
      300,
      7501,
      2417,
      354
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "cyclic_group",
    "text": "The group can be cyclic, i.e. all elements of the group can be generated by repeatedly applying the group law.",
    "token_ids": [
      758,
      2746,
      566,
      364,
      61652,
      44,
      1312,
      5318,
      46,
      679,
      6387,
      300,
      275,
      2746,
      566,
      364,
      9689,
      531,
      29211,
      16322,
      275,
      2746,
      2749,
      46
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "additive_notation",
    "text": "[a]P to represent P+P+ .... + P, applying the group law a times, i.e. the scalar multiplication.",
    "token_ids": [
      65323,
      93,
      80,
      301,
      3137,
      371,
      114010,
      43,
      51262,
      1349,
      371,
      44,
      16322,
      275,
      2746,
      2749,
      258,
      3539,
      44,
      1312,
      5318,
      46,
      275,
      36133,
      45271,
      46
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_prover",
    "text": "Prover computes witness polynomial w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      1661,
      415,
      92329,
      12583,
      36552,
      282,
      4704,
      44,
      1443,
      41,
      409,
      359,
      112,
      4704,
      41,
      661,
      273,
      21290,
      2261,
      1180,
      359,
      120,
      27658,
      41
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_verifier",
    "text": "Verifier checks proof.(œÑ-z) = p(œÑ) - y using bilinear pairing",
    "token_ids": [
      131859,
      18899,
      11067,
      20210,
      20669,
      27658,
      41,
      409,
      273,
      40,
      20669,
      41,
      661,
      330,
      1818,
      179816,
      70489
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_fiat_shamir",
    "text": "To make the protocol non-interactive, z may be computed via the Fiat-Shamir heuristic.",
    "token_ids": [
      2455,
      1454,
      275,
      10345,
      2295,
      195515,
      44,
      1443,
      992,
      364,
      26826,
      4247,
      275,
      88011,
      46583,
      322,
      357,
      98364,
      46
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_schwartz",
    "text": "According to the Schwartz-zippel Lemma it is cryptographically unlikely that this equation holds",
    "token_ids": [
      13428,
      301,
      275,
      78833,
      27658,
      11994,
      295,
      32660,
      412,
      355,
      13018,
      69773,
      23788,
      389,
      546,
      12026,
      13364
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_srs_g1",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau, with a bounded degree n-1",
    "token_ids": [
      115,
      11686,
      11199,
      49,
      58,
      3724,
      49,
      93,
      73173,
      44,
      791,
      20669,
      93,
      73173,
      44,
      791,
      20669,
      21038,
      93,
      73173,
      44,
      4831,
      791,
      20669,
      39379,
      191,
      39379,
      187,
      83184,
      93,
      73173,
      93,
      964,
      3431,
      17370,
      300,
      53562,
      44,
      418,
      258,
      36085,
      6959,
      311,
      45,
      49
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_srs_g2",
    "text": "srs_g2: [[1]‚ÇÇ, [œÑ]‚ÇÇ]",
    "token_ids": [
      115,
      11686,
      11199,
      50,
      58,
      3724,
      49,
      93,
      67671,
      44,
      791,
      20669,
      93,
      67671,
      93
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_trust_setup",
    "text": "œÑ and its powers are secrets that no one knows, we only work with [œÑ‚Å±]‚ÇÅ and [œÑ]‚ÇÇ not with œÑ directly",
    "token_ids": [
      20669,
      306,
      1072,
      17370,
      457,
      29824,
      389,
      687,
      841,
      10824,
      44,
      563,
      1245,
      1002,
      418,
      791,
      20669,
      39379,
      177,
      93,
      73173,
      306,
      791,
      20669,
      93,
      67671,
      516,
      418,
      32345,
      6467
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  },
  {
    "name": "pcs_monomial",
    "text": "p(x) = blob‚ÇÄ + blob‚ÇÅ x + blob‚ÇÇ x¬≤ + ... + blob‚Çô‚Çã‚ÇÅ x‚Åø‚Åª¬π",
    "token_ids": [
      112,
      4704,
      41,
      409,
      66949,
      177094,
      1349,
      66949,
      73173,
      1905,
      1349,
      66949,
      67671,
      1905,
      21038,
      1349,
      4831,
      1349,
      66949,
      30288,
      153,
      30288,
      139,
      73173,
      1905,
      39379,
      191,
      39379,
      187,
      83184
    ],
    "tokenizer": "minimax-m2.1-tokenizer.json"
  }
]