[
  {
    "name": "chinese",
    "text": "‰Ω†Â•Ω‰∏ñÁïå",
    "token_ids": [
      30594,
      3427
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "japanese",
    "text": "„Åì„Çì„Å´„Å°„ÅØ",
    "token_ids": [
      4549,
      7245,
      2298,
      12457,
      2841
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "korean",
    "text": "ÏïàÎÖïÌïòÏÑ∏Ïöî ÏÑ∏Í≥Ñ",
    "token_ids": [
      31404,
      11939,
      246,
      4567,
      73527,
      76878
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "russian",
    "text": "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "token_ids": [
      24797,
      8919,
      74779
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "hebrew",
    "text": "◊©◊ú◊ï◊ù ◊¢◊ï◊ú◊ù",
    "token_ids": [
      8793,
      2328,
      16467,
      6166,
      41141
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "khmer",
    "text": "·ûü·ûΩ·ûü·üí·ûè·û∏·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ",
    "token_ids": [
      89301,
      7611,
      124,
      89301,
      35188,
      240,
      7611,
      119,
      7611,
      247,
      98407,
      7611,
      248,
      7611,
      247,
      118074,
      17767,
      229,
      70285
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "emoji",
    "text": "Hello üåç World! üéâ",
    "token_ids": [
      19923,
      73369,
      238,
      4495,
      3,
      7351,
      239,
      234
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "mixed_cjk",
    "text": "Hello ‰∏ñÁïå „Åì„Çì„Å´„Å°„ÅØ ÏïàÎÖï",
    "token_ids": [
      19923,
      223,
      3427,
      223,
      4549,
      7245,
      2298,
      12457,
      2841,
      38880,
      11939,
      246
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "hello_world",
    "text": "Hello, world!",
    "token_ids": [
      19923,
      14,
      2058,
      3
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "greek_basic",
    "text": "ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Œ∫Œ±Œπ ŒºŒ±Œ∏Œ∑ŒºŒ±œÑŒπŒ∫Œ¨",
    "token_ids": [
      40435,
      43595,
      20975,
      16700,
      86049,
      23519,
      12271,
      95633
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "greek_alphabet",
    "text": "Œë Œ± Œí Œ≤ Œì Œ≥ Œî Œ¥ Œï Œµ Œñ Œ∂ Œó Œ∑ Œò Œ∏ Œô Œπ Œö Œ∫ Œõ Œª Œú Œº Œù ŒΩ Œû Œæ Œü Œø Œ† œÄ Œ° œÅ Œ£ œÉ Œ§ œÑ Œ• œÖ Œ¶ œÜ Œß œá Œ® œà Œ© œâ",
    "token_ids": [
      32493,
      6056,
      34512,
      12962,
      25938,
      12935,
      17405,
      11512,
      19217,
      7972,
      123351,
      66903,
      51192,
      20250,
      59529,
      15279,
      36392,
      54699,
      17011,
      8055,
      38634,
      15039,
      20570,
      7695,
      30097,
      19397,
      1247,
      255,
      52063,
      32184,
      15107,
      17671,
      7014,
      57485,
      30688,
      18744,
      6931,
      27522,
      4428,
      93197,
      38508,
      38491,
      18338,
      56914,
      24151,
      112442,
      52871,
      35571,
      26201
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_endo",
    "text": "Check the Endomorphism for p mod 3 == 1",
    "token_ids": [
      15205,
      270,
      8516,
      56389,
      362,
      280,
      1267,
      223,
      21,
      2606,
      223,
      19
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_cube_roots",
    "text": "x¬≥‚àí1=0 <=> (x‚àí1)(x¬≤+x+1) = 0, if x != 1, x solves (x¬≤+x+1) = 0 <=> x = (-1¬±‚àö3)/2",
    "token_ids": [
      90,
      5826,
      4023,
      19,
      31,
      18,
      8593,
      32,
      343,
      90,
      4023,
      19,
      5796,
      90,
      1628,
      34705,
      13,
      19,
      11,
      438,
      223,
      18,
      14,
      855,
      1527,
      6269,
      223,
      19,
      14,
      1527,
      83029,
      343,
      90,
      1628,
      34705,
      13,
      19,
      11,
      438,
      223,
      18,
      8593,
      32,
      1527,
      438,
      10435,
      19,
      14594,
      13612,
      21,
      8930,
      20
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_curve",
    "text": "y¬≤ = x¬≥ + b, and y¬≤ = (xùúë)¬≥ + b <=> y¬≤ = x¬≥ + b (with ùúë¬≥ == 1) so we are still on the curve",
    "token_ids": [
      91,
      1628,
      438,
      1527,
      5826,
      940,
      291,
      14,
      305,
      383,
      1628,
      438,
      343,
      90,
      95203,
      11,
      5826,
      940,
      291,
      8593,
      32,
      383,
      1628,
      438,
      1527,
      5826,
      940,
      291,
      343,
      6135,
      223,
      95203,
      5826,
      2606,
      223,
      19,
      11,
      832,
      579,
      477,
      2413,
      377,
      270,
      13104
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_lambda",
    "text": "Œª·µ©¬≤ + Œª·µ© + 1 ‚â° 0 (mod r) and ùúë¬≤ + ùúë + 1 ‚â° 0 (mod p)",
    "token_ids": [
      3422,
      160,
      116,
      105,
      1628,
      940,
      15039,
      160,
      116,
      105,
      940,
      223,
      19,
      56930,
      223,
      18,
      343,
      5158,
      494,
      11,
      305,
      223,
      95203,
      1628,
      940,
      223,
      95203,
      940,
      223,
      19,
      56930,
      223,
      18,
      343,
      5158,
      280,
      11
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_scalar",
    "text": "[a]P to represent P+P+ .... + P",
    "token_ids": [
      36495,
      63,
      50,
      304,
      3293,
      380,
      98495,
      13,
      36757,
      940,
      380
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_bilinear",
    "text": "e: ùîæ1 x ùîæ2 -> ùîæt that map is bilinear e([a]P, [b]Q) = e(P, Q)·µÉ·µá",
    "token_ids": [
      71,
      28,
      83689,
      245,
      125,
      19,
      1527,
      83689,
      245,
      125,
      20,
      6248,
      83689,
      245,
      125,
      86,
      396,
      6403,
      344,
      16731,
      47883,
      312,
      10425,
      67,
      63,
      50,
      14,
      764,
      68,
      63,
      51,
      11,
      438,
      312,
      17677,
      14,
      1646,
      11,
      160,
      116,
      228,
      160,
      116,
      232
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_srs",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau",
    "token_ids": [
      85,
      8492,
      15810,
      19,
      28,
      12955,
      19,
      63,
      90534,
      14,
      764,
      1806,
      63,
      90534,
      14,
      764,
      1806,
      1628,
      63,
      90534,
      14,
      4588,
      764,
      1806,
      102359,
      123520,
      48283,
      63,
      90534,
      63,
      990,
      3252,
      14589,
      294,
      67656
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_commitment",
    "text": "commit(srs_g1, blob) -> commitment C = ‚àë blob·µ¢.srs_g1·µ¢ = ‚àë [blob·µ¢.œÑ‚Å±]‚ÇÅ = [p(œÑ)]‚ÇÅ",
    "token_ids": [
      55736,
      3982,
      8492,
      15810,
      19,
      14,
      112566,
      11,
      6248,
      12438,
      345,
      438,
      52621,
      112566,
      160,
      116,
      98,
      2349,
      8492,
      15810,
      19,
      160,
      116,
      98,
      438,
      52621,
      764,
      3778,
      924,
      160,
      116,
      98,
      16,
      1806,
      15371,
      112,
      63,
      90534,
      438,
      764,
      82,
      10,
      1806,
      14245,
      90534
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_witness",
    "text": "w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      89,
      4042,
      14,
      961,
      11,
      438,
      343,
      82,
      4042,
      11,
      565,
      280,
      19611,
      2542,
      1492,
      343,
      90,
      26831,
      11
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_verification",
    "text": "e([proof]‚ÇÅ, [œÑ]‚ÇÇ - [z]‚ÇÇ) = e(C - [y]‚ÇÅ, [1]‚ÇÇ)",
    "token_ids": [
      71,
      10425,
      36552,
      63,
      90534,
      14,
      764,
      1806,
      63,
      59853,
      565,
      764,
      92,
      63,
      59853,
      11,
      438,
      312,
      14128,
      565,
      764,
      91,
      63,
      90534,
      14,
      764,
      19,
      63,
      59853,
      11
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "ecc_lagrange",
    "text": "œâ ‚àà ùîΩr a root of unity of order n, i.e. œâ‚Åø = 1",
    "token_ids": [
      5583,
      15060,
      83689,
      245,
      124,
      84,
      260,
      4798,
      294,
      24644,
      294,
      2496,
      313,
      14,
      1008,
      4987,
      16,
      26201,
      102359,
      438,
      223,
      19
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "group_basics",
    "text": "A group is a set of elements with a binary operation called the group law with a neutral element and an inverse.",
    "token_ids": [
      35,
      2740,
      344,
      260,
      1341,
      294,
      5486,
      418,
      260,
      11680,
      7408,
      3252,
      270,
      2740,
      2950,
      418,
      260,
      15744,
      4885,
      305,
      411,
      24683,
      16
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "field_basics",
    "text": "A field is a set of elements with two group laws, addition and multiplication, with corresponding inverse properties.",
    "token_ids": [
      35,
      2994,
      344,
      260,
      1341,
      294,
      5486,
      418,
      1234,
      2740,
      8384,
      14,
      3012,
      305,
      21000,
      14,
      418,
      9453,
      24683,
      5470,
      16
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "finite_field",
    "text": "ùîΩr is a finite-field of prime order r",
    "token_ids": [
      3128,
      245,
      124,
      84,
      344,
      260,
      19823,
      42208,
      294,
      4309,
      2496,
      494
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "cyclic_group",
    "text": "The group can be cyclic, i.e. all elements of the group can be generated by repeatedly applying the group law.",
    "token_ids": [
      671,
      2740,
      588,
      366,
      46597,
      14,
      1008,
      4987,
      16,
      710,
      5486,
      294,
      270,
      2740,
      588,
      366,
      9846,
      513,
      28131,
      15666,
      270,
      2740,
      2950,
      16
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "additive_notation",
    "text": "[a]P to represent P+P+ .... + P, applying the group law a times, i.e. the scalar multiplication.",
    "token_ids": [
      36495,
      63,
      50,
      304,
      3293,
      380,
      98495,
      13,
      36757,
      940,
      380,
      14,
      15666,
      270,
      2740,
      2950,
      260,
      2734,
      14,
      1008,
      4987,
      16,
      270,
      42564,
      21000,
      16
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_prover",
    "text": "Prover computes witness polynomial w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      2497,
      432,
      89946,
      11654,
      23388,
      281,
      4042,
      14,
      961,
      11,
      438,
      343,
      82,
      4042,
      11,
      565,
      280,
      19611,
      2542,
      1492,
      343,
      90,
      26831,
      11
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_verifier",
    "text": "Verifier checks proof.(œÑ-z) = p(œÑ) - y using bilinear pairing",
    "token_ids": [
      15899,
      8748,
      21323,
      12745,
      14970,
      1806,
      26831,
      11,
      438,
      280,
      10,
      1806,
      11,
      565,
      383,
      1812,
      16731,
      47883,
      60122
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_fiat_shamir",
    "text": "To make the protocol non-interactive, z may be computed via the Fiat-Shamir heuristic.",
    "token_ids": [
      3054,
      1635,
      270,
      12093,
      2408,
      33240,
      6615,
      14,
      961,
      1142,
      366,
      25126,
      5566,
      270,
      447,
      5233,
      42688,
      356,
      388,
      77654,
      16
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_schwartz",
    "text": "According to the Schwartz-zippel Lemma it is cryptographically unlikely that this equation holds",
    "token_ids": [
      12688,
      304,
      270,
      64208,
      15,
      6030,
      82658,
      50619,
      436,
      344,
      15763,
      57673,
      22865,
      396,
      566,
      6550,
      12927
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_srs_g1",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau, with a bounded degree n-1",
    "token_ids": [
      85,
      8492,
      15810,
      19,
      28,
      12955,
      19,
      63,
      90534,
      14,
      764,
      1806,
      63,
      90534,
      14,
      764,
      1806,
      1628,
      63,
      90534,
      14,
      4588,
      764,
      1806,
      102359,
      123520,
      48283,
      63,
      90534,
      63,
      990,
      3252,
      14589,
      294,
      67656,
      14,
      418,
      260,
      37465,
      6954,
      313,
      15,
      19
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_srs_g2",
    "text": "srs_g2: [[1]‚ÇÇ, [œÑ]‚ÇÇ]",
    "token_ids": [
      85,
      8492,
      15810,
      20,
      28,
      12955,
      19,
      63,
      59853,
      14,
      764,
      1806,
      63,
      59853,
      63
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_trust_setup",
    "text": "œÑ and its powers are secrets that no one knows, we only work with [œÑ‚Å±]‚ÇÅ and [œÑ]‚ÇÇ not with œÑ directly",
    "token_ids": [
      1806,
      305,
      1009,
      14589,
      477,
      31760,
      396,
      1119,
      834,
      11457,
      14,
      579,
      1353,
      1116,
      418,
      764,
      1806,
      15371,
      112,
      63,
      90534,
      305,
      764,
      1806,
      63,
      59853,
      554,
      418,
      4428,
      6578
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  },
  {
    "name": "pcs_monomial",
    "text": "p(x) = blob‚ÇÄ + blob‚ÇÅ x + blob‚ÇÇ x¬≤ + ... + blob‚Çô‚Çã‚ÇÅ x‚Åø‚Åª¬π",
    "token_ids": [
      82,
      4042,
      11,
      438,
      112566,
      26558,
      225,
      940,
      112566,
      90534,
      1527,
      940,
      112566,
      59853,
      1527,
      1628,
      940,
      4588,
      940,
      112566,
      26558,
      250,
      26558,
      236,
      90534,
      1527,
      102359,
      123520,
      48283
    ],
    "tokenizer": "step-3.5-flash-tokenizer.json"
  }
]