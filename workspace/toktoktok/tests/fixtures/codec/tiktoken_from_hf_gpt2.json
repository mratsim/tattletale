[
  {
    "name": "chinese",
    "text": "‰Ω†Â•Ω‰∏ñÁïå",
    "token_ids": [
      19526,
      254,
      25001,
      121,
      10310,
      244,
      45911,
      234
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "japanese",
    "text": "„Åì„Çì„Å´„Å°„ÅØ",
    "token_ids": [
      46036,
      22174,
      28618,
      2515,
      94,
      31676
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "korean",
    "text": "ÏïàÎÖïÌïòÏÑ∏Ïöî ÏÑ∏Í≥Ñ",
    "token_ids": [
      168,
      243,
      230,
      167,
      227,
      243,
      47991,
      246,
      168,
      226,
      116,
      168,
      248,
      242,
      23821,
      226,
      116,
      166,
      111,
      226
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "russian",
    "text": "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "token_ids": [
      140,
      253,
      21169,
      18849,
      38857,
      16843,
      20375,
      12466,
      120,
      18849,
      21169
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "hebrew",
    "text": "◊©◊ú◊ï◊ù ◊¢◊ï◊ú◊ù",
    "token_ids": [
      50227,
      40010,
      27072,
      147,
      251,
      14360,
      95,
      27072,
      40010,
      147,
      251
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "khmer",
    "text": "·ûü·ûΩ·ûü·üí·ûè·û∏·ûñ·û∑·ûó·ûñ·ûõ·üÑ·ûÄ",
    "token_ids": [
      157,
      252,
      253,
      157,
      252,
      121,
      157,
      252,
      253,
      157,
      253,
      240,
      157,
      252,
      237,
      157,
      252,
      116,
      157,
      252,
      244,
      157,
      252,
      115,
      157,
      252,
      245,
      157,
      252,
      244,
      157,
      252,
      249,
      157,
      253,
      226,
      157,
      252,
      222
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "emoji",
    "text": "Hello üåç World! üéâ",
    "token_ids": [
      15496,
      12520,
      234,
      235,
      2159,
      0,
      12520,
      236,
      231
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "mixed_cjk",
    "text": "Hello ‰∏ñÁïå „Åì„Çì„Å´„Å°„ÅØ ÏïàÎÖï",
    "token_ids": [
      15496,
      220,
      10310,
      244,
      45911,
      234,
      23294,
      241,
      22174,
      28618,
      2515,
      94,
      31676,
      23821,
      243,
      230,
      167,
      227,
      243
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "hello_world",
    "text": "Hello, world!",
    "token_ids": [
      15496,
      11,
      995,
      0
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "greek_basic",
    "text": "ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ Œ∫Œ±Œπ ŒºŒ±Œ∏Œ∑ŒºŒ±œÑŒπŒ∫Œ¨",
    "token_ids": [
      138,
      243,
      39377,
      39377,
      138,
      115,
      26180,
      29945,
      43000,
      138,
      105,
      7377,
      118,
      17394,
      29945,
      18919,
      17394,
      138,
      116,
      138,
      115,
      34703,
      17394,
      32830,
      29945,
      43000,
      138,
      105
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "greek_alphabet",
    "text": "Œë Œ± Œí Œ≤ Œì Œ≥ Œî Œ¥ Œï Œµ Œñ Œ∂ Œó Œ∑ Œò Œ∏ Œô Œπ Œö Œ∫ Œõ Œª Œú Œº Œù ŒΩ Œû Œæ Œü Œø Œ† œÄ Œ° œÅ Œ£ œÉ Œ§ œÑ Œ• œÖ Œ¶ œÜ Œß œá Œ® œà Œ© œâ",
    "token_ids": [
      138,
      239,
      26367,
      7377,
      240,
      27169,
      7377,
      241,
      7377,
      111,
      37455,
      7377,
      112,
      7377,
      243,
      7377,
      113,
      7377,
      244,
      7377,
      114,
      7377,
      245,
      7377,
      115,
      7377,
      246,
      7377,
      116,
      7377,
      247,
      7377,
      117,
      7377,
      248,
      7377,
      118,
      7377,
      249,
      7377,
      119,
      7377,
      250,
      18919,
      7377,
      251,
      7377,
      121,
      7377,
      252,
      7377,
      122,
      7377,
      253,
      7377,
      123,
      7377,
      254,
      18074,
      222,
      7377,
      94,
      18074,
      223,
      7377,
      96,
      18074,
      225,
      7377,
      97,
      46651,
      7377,
      98,
      18074,
      227,
      7377,
      99,
      18074,
      228,
      7377,
      100,
      18074,
      229,
      7377,
      101,
      18074,
      230,
      7377,
      102,
      18074,
      231
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_endo",
    "text": "Check the Endomorphism for p mod 3 == 1",
    "token_ids": [
      9787,
      262,
      5268,
      25831,
      1042,
      329,
      279,
      953,
      513,
      6624,
      352
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_cube_roots",
    "text": "x¬≥‚àí1=0 <=> (x‚àí1)(x¬≤+x+1) = 0, if x != 1, x solves (x¬≤+x+1) = 0 <=> x = (-1¬±‚àö3)/2",
    "token_ids": [
      87,
      126,
      111,
      14095,
      16,
      28,
      15,
      1279,
      14804,
      357,
      87,
      14095,
      16,
      5769,
      87,
      31185,
      10,
      87,
      10,
      16,
      8,
      796,
      657,
      11,
      611,
      2124,
      14512,
      352,
      11,
      2124,
      39107,
      357,
      87,
      31185,
      10,
      87,
      10,
      16,
      8,
      796,
      657,
      1279,
      14804,
      2124,
      796,
      13841,
      16,
      22519,
      24861,
      248,
      18,
      20679,
      17
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_curve",
    "text": "y¬≤ = x¬≥ + b, and y¬≤ = (xùúë)¬≥ + b <=> y¬≤ = x¬≥ + b (with ùúë¬≥ == 1) so we are still on the curve",
    "token_ids": [
      88,
      31185,
      796,
      2124,
      126,
      111,
      1343,
      275,
      11,
      290,
      331,
      31185,
      796,
      357,
      87,
      47728,
      250,
      239,
      8,
      126,
      111,
      1343,
      275,
      1279,
      14804,
      331,
      31185,
      796,
      2124,
      126,
      111,
      1343,
      275,
      357,
      4480,
      220,
      47728,
      250,
      239,
      126,
      111,
      6624,
      352,
      8,
      523,
      356,
      389,
      991,
      319,
      262,
      12133
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_lambda",
    "text": "Œª·µ©¬≤ + Œª·µ© + 1 ‚â° 0 (mod r) and ùúë¬≤ + ùúë + 1 ‚â° 0 (mod p)",
    "token_ids": [
      39377,
      39611,
      102,
      31185,
      1343,
      7377,
      119,
      39611,
      102,
      1343,
      352,
      38243,
      657,
      357,
      4666,
      374,
      8,
      290,
      220,
      47728,
      250,
      239,
      31185,
      1343,
      220,
      47728,
      250,
      239,
      1343,
      352,
      38243,
      657,
      357,
      4666,
      279,
      8
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_scalar",
    "text": "[a]P to represent P+P+ .... + P",
    "token_ids": [
      58,
      64,
      60,
      47,
      284,
      2380,
      350,
      10,
      47,
      10,
      19424,
      1343,
      350
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_bilinear",
    "text": "e: ùîæ1 x ùîæ2 -> ùîæt that map is bilinear e([a]P, [b]Q) = e(P, Q)·µÉ·µá",
    "token_ids": [
      68,
      25,
      220,
      47728,
      242,
      122,
      16,
      2124,
      220,
      47728,
      242,
      122,
      17,
      4613,
      220,
      47728,
      242,
      122,
      83,
      326,
      3975,
      318,
      47027,
      259,
      451,
      304,
      26933,
      64,
      60,
      47,
      11,
      685,
      65,
      60,
      48,
      8,
      796,
      304,
      7,
      47,
      11,
      1195,
      8,
      39611,
      225,
      39611,
      229
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_srs",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau",
    "token_ids": [
      82,
      3808,
      62,
      70,
      16,
      25,
      16410,
      16,
      60,
      158,
      224,
      223,
      11,
      685,
      32830,
      60,
      158,
      224,
      223,
      11,
      685,
      32830,
      31185,
      60,
      158,
      224,
      223,
      11,
      2644,
      685,
      32830,
      46256,
      123,
      46256,
      119,
      126,
      117,
      60,
      158,
      224,
      223,
      60,
      635,
      1444,
      5635,
      286,
      256,
      559
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_commitment",
    "text": "commit(srs_g1, blob) -> commitment C = ‚àë blob·µ¢.srs_g1·µ¢ = ‚àë [blob·µ¢.œÑ‚Å±]‚ÇÅ = [p(œÑ)]‚ÇÅ",
    "token_ids": [
      41509,
      7,
      82,
      3808,
      62,
      70,
      16,
      11,
      44812,
      8,
      4613,
      7901,
      327,
      796,
      18872,
      239,
      44812,
      39611,
      95,
      13,
      82,
      3808,
      62,
      70,
      16,
      39611,
      95,
      796,
      18872,
      239,
      685,
      2436,
      672,
      39611,
      95,
      13,
      32830,
      46256,
      109,
      60,
      158,
      224,
      223,
      796,
      685,
      79,
      7,
      32830,
      15437,
      158,
      224,
      223
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_witness",
    "text": "w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      86,
      7,
      87,
      11,
      1976,
      8,
      796,
      357,
      79,
      7,
      87,
      8,
      532,
      279,
      7,
      89,
      4008,
      1220,
      357,
      87,
      12,
      89,
      8
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_verification",
    "text": "e([proof]‚ÇÅ, [œÑ]‚ÇÇ - [z]‚ÇÇ) = e(C - [y]‚ÇÅ, [1]‚ÇÇ)",
    "token_ids": [
      68,
      26933,
      13288,
      60,
      158,
      224,
      223,
      11,
      685,
      32830,
      60,
      158,
      224,
      224,
      532,
      685,
      89,
      60,
      158,
      224,
      224,
      8,
      796,
      304,
      7,
      34,
      532,
      685,
      88,
      60,
      158,
      224,
      223,
      11,
      685,
      16,
      60,
      158,
      224,
      224,
      8
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "ecc_lagrange",
    "text": "œâ ‚àà ùîΩr a root of unity of order n, i.e. œâ‚Åø = 1",
    "token_ids": [
      49535,
      18872,
      230,
      220,
      47728,
      242,
      121,
      81,
      257,
      6808,
      286,
      14111,
      286,
      1502,
      299,
      11,
      1312,
      13,
      68,
      13,
      18074,
      231,
      46256,
      123,
      796,
      352
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "group_basics",
    "text": "A group is a set of elements with a binary operation called the group law with a neutral element and an inverse.",
    "token_ids": [
      32,
      1448,
      318,
      257,
      900,
      286,
      4847,
      351,
      257,
      13934,
      4905,
      1444,
      262,
      1448,
      1099,
      351,
      257,
      8500,
      5002,
      290,
      281,
      34062,
      13
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "field_basics",
    "text": "A field is a set of elements with two group laws, addition and multiplication, with corresponding inverse properties.",
    "token_ids": [
      32,
      2214,
      318,
      257,
      900,
      286,
      4847,
      351,
      734,
      1448,
      3657,
      11,
      3090,
      290,
      48473,
      11,
      351,
      11188,
      34062,
      6608,
      13
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "finite_field",
    "text": "ùîΩr is a finite-field of prime order r",
    "token_ids": [
      47728,
      242,
      121,
      81,
      318,
      257,
      27454,
      12,
      3245,
      286,
      6994,
      1502,
      374
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "cyclic_group",
    "text": "The group can be cyclic, i.e. all elements of the group can be generated by repeatedly applying the group law.",
    "token_ids": [
      464,
      1448,
      460,
      307,
      11700,
      291,
      11,
      1312,
      13,
      68,
      13,
      477,
      4847,
      286,
      262,
      1448,
      460,
      307,
      7560,
      416,
      7830,
      11524,
      262,
      1448,
      1099,
      13
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "additive_notation",
    "text": "[a]P to represent P+P+ .... + P, applying the group law a times, i.e. the scalar multiplication.",
    "token_ids": [
      58,
      64,
      60,
      47,
      284,
      2380,
      350,
      10,
      47,
      10,
      19424,
      1343,
      350,
      11,
      11524,
      262,
      1448,
      1099,
      257,
      1661,
      11,
      1312,
      13,
      68,
      13,
      262,
      16578,
      283,
      48473,
      13
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_prover",
    "text": "Prover computes witness polynomial w(x, z) = (p(x) - p(z)) / (x-z)",
    "token_ids": [
      2964,
      332,
      552,
      1769,
      4973,
      745,
      6213,
      49070,
      266,
      7,
      87,
      11,
      1976,
      8,
      796,
      357,
      79,
      7,
      87,
      8,
      532,
      279,
      7,
      89,
      4008,
      1220,
      357,
      87,
      12,
      89,
      8
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_verifier",
    "text": "Verifier checks proof.(œÑ-z) = p(œÑ) - y using bilinear pairing",
    "token_ids": [
      13414,
      7483,
      8794,
      6617,
      12195,
      32830,
      12,
      89,
      8,
      796,
      279,
      7,
      32830,
      8,
      532,
      331,
      1262,
      47027,
      259,
      451,
      27356
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_fiat_shamir",
    "text": "To make the protocol non-interactive, z may be computed via the Fiat-Shamir heuristic.",
    "token_ids": [
      2514,
      787,
      262,
      8435,
      1729,
      12,
      3849,
      5275,
      11,
      1976,
      743,
      307,
      29231,
      2884,
      262,
      23327,
      12,
      43478,
      343,
      339,
      27915,
      13
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_schwartz",
    "text": "According to the Schwartz-zippel Lemma it is cryptographically unlikely that this equation holds",
    "token_ids": [
      4821,
      284,
      262,
      28672,
      12,
      89,
      3974,
      417,
      20607,
      2611,
      340,
      318,
      8194,
      33145,
      7485,
      326,
      428,
      16022,
      6622
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_srs_g1",
    "text": "srs_g1: [[1]‚ÇÅ, [œÑ]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ... [œÑ‚Åø‚Åª¬π]‚ÇÅ] also called powers of tau, with a bounded degree n-1",
    "token_ids": [
      82,
      3808,
      62,
      70,
      16,
      25,
      16410,
      16,
      60,
      158,
      224,
      223,
      11,
      685,
      32830,
      60,
      158,
      224,
      223,
      11,
      685,
      32830,
      31185,
      60,
      158,
      224,
      223,
      11,
      2644,
      685,
      32830,
      46256,
      123,
      46256,
      119,
      126,
      117,
      60,
      158,
      224,
      223,
      60,
      635,
      1444,
      5635,
      286,
      256,
      559,
      11,
      351,
      257,
      49948,
      4922,
      299,
      12,
      16
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_srs_g2",
    "text": "srs_g2: [[1]‚ÇÇ, [œÑ]‚ÇÇ]",
    "token_ids": [
      82,
      3808,
      62,
      70,
      17,
      25,
      16410,
      16,
      60,
      158,
      224,
      224,
      11,
      685,
      32830,
      60,
      158,
      224,
      224,
      60
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_trust_setup",
    "text": "œÑ and its powers are secrets that no one knows, we only work with [œÑ‚Å±]‚ÇÅ and [œÑ]‚ÇÇ not with œÑ directly",
    "token_ids": [
      32830,
      290,
      663,
      5635,
      389,
      13141,
      326,
      645,
      530,
      4206,
      11,
      356,
      691,
      670,
      351,
      685,
      32830,
      46256,
      109,
      60,
      158,
      224,
      223,
      290,
      685,
      32830,
      60,
      158,
      224,
      224,
      407,
      351,
      46651,
      3264
    ],
    "tokenizer": "gpt2-tokenizer.json"
  },
  {
    "name": "pcs_monomial",
    "text": "p(x) = blob‚ÇÄ + blob‚ÇÅ x + blob‚ÇÇ x¬≤ + ... + blob‚Çô‚Çã‚ÇÅ x‚Åø‚Åª¬π",
    "token_ids": [
      79,
      7,
      87,
      8,
      796,
      44812,
      158,
      224,
      222,
      1343,
      44812,
      158,
      224,
      223,
      2124,
      1343,
      44812,
      158,
      224,
      224,
      2124,
      31185,
      1343,
      2644,
      1343,
      44812,
      158,
      224,
      247,
      158,
      224,
      233,
      158,
      224,
      223,
      2124,
      46256,
      123,
      46256,
      119,
      126,
      117
    ],
    "tokenizer": "gpt2-tokenizer.json"
  }
]