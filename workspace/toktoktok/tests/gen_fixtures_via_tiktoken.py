#!/usr/bin/env python3
"""
Generate fixture files for tokenizer testing via tiktoken.

Fixtures are JSON files with the format:
{
    "text": "Some example text",
    "token_ids": [10, 234, 6909, ..., 99],
    "tokenizer": "tiktoken_r50k_base.tiktoken"
}
"""

import json
import base64
from pathlib import Path
from typing import Dict, List, Any
import tiktoken

import hf_to_tiktoken

TEST_DIR = Path(__file__).parent.resolve()
TOKENIZERS_DIR = TEST_DIR / "tokenizers"
FIXTURES_DIR = TEST_DIR / "fixtures" / "codec"
FIXTURES_DIR.mkdir(parents=True, exist_ok=True)

TIKTOKEN_FILES = {
    "r50k_base": "r50k_base.tiktoken",
    "p50k_base": "p50k_base.tiktoken",
    "cl100k_base": "cl100k_base.tiktoken",
    "o200k_base": "o200k_base.tiktoken",
    "kimik2.5": "kimik2.5.tiktoken",
}

HF_FILES = {
    "gpt2": "gpt2-tokenizer.json",
    "llama3": "llama3-tokenizer.json",
    "minimax-m2.1": "minimax-m2.1-tokenizer.json",
    "glm-4.7": "glm-4.7-tokenizer.json",
    "exaone": "exaone-tokenizer.json",
    "step-3.5-flash": "step-3.5-flash-tokenizer.json",
}


def parse_tiktoken(path: Path) -> Dict[bytes, int]:
    """Parse a tiktoken file and return token->rank mapping."""
    mergeable_ranks = {}
    with open(path, "r") as f:
        content = f.read()
    for line in content.strip().split("\n"):
        parts = line.split()
        if len(parts) == 2:
            try:
                token_bytes = base64.b64decode(parts[0])
                rank = int(parts[1])
                mergeable_ranks[token_bytes] = rank
            except:
                pass
    return mergeable_ranks


def generate_fixture(
    text: str, tokenizer_name: str, encoding: tiktoken.Encoding
) -> Dict[str, Any]:
    """Generate a single fixture entry."""
    token_ids = encoding.encode_ordinary(text)
    return {"text": text, "token_ids": token_ids, "tokenizer": tokenizer_name}


def get_test_texts() -> List[tuple[str, str]]:
    """Get all test texts for fixture generation."""
    texts = []

    # From test_roundtrip_tiktokenizer.nim
    texts.extend(
        [
            ("chinese", "ä½ å¥½ä¸–ç•Œ"),
            ("japanese", "ã“ã‚“ã«ã¡ã¯"),
            ("korean", "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„"),
            ("russian", "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€"),
            ("hebrew", "×©×œ×•× ×¢×•×œ×"),
            ("khmer", "áŸá½áŸáŸ’áá¸á–á·á—á–á›áŸ„á€"),
            ("emoji", "Hello ğŸŒ World! ğŸ‰"),
            ("mixed_cjk", "Hello ä¸–ç•Œ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•"),
            ("hello_world", "Hello, world!"),
        ]
    )

    # Greek texts
    texts.extend(
        [
            ("greek_basic", "Î•Î»Î»Î·Î½Î¹ÎºÎ¬ ÎºÎ±Î¹ Î¼Î±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ¬"),
            (
                "greek_alphabet",
                "Î‘ Î± Î’ Î² Î“ Î³ Î” Î´ Î• Îµ Î– Î¶ Î— Î· Î˜ Î¸ Î™ Î¹ Îš Îº Î› Î» Îœ Î¼ Î Î½ Î Î¾ ÎŸ Î¿ Î  Ï€ Î¡ Ï Î£ Ïƒ Î¤ Ï„ Î¥ Ï… Î¦ Ï† Î§ Ï‡ Î¨ Ïˆ Î© Ï‰",
            ),
        ]
    )

    # Mathematical symbols (ECC documentation excerpts)
    texts.extend(
        [
            ("ecc_endo", "Check the Endomorphism for p mod 3 == 1"),
            (
                "ecc_cube_roots",
                "xÂ³âˆ’1=0 <=> (xâˆ’1)(xÂ²+x+1) = 0, if x != 1, x solves (xÂ²+x+1) = 0 <=> x = (-1Â±âˆš3)/2",
            ),
            (
                "ecc_curve",
                "yÂ² = xÂ³ + b, and yÂ² = (xğœ‘)Â³ + b <=> yÂ² = xÂ³ + b (with ğœ‘Â³ == 1) so we are still on the curve",
            ),
            ("ecc_lambda", "Î»áµ©Â² + Î»áµ© + 1 â‰¡ 0 (mod r) and ğœ‘Â² + ğœ‘ + 1 â‰¡ 0 (mod p)"),
            ("ecc_scalar", "[a]P to represent P+P+ .... + P"),
            (
                "ecc_bilinear",
                "e: ğ”¾1 x ğ”¾2 -> ğ”¾t that map is bilinear e([a]P, [b]Q) = e(P, Q)áµƒáµ‡",
            ),
            (
                "ecc_srs",
                "srs_g1: [[1]â‚, [Ï„]â‚, [Ï„Â²]â‚, ... [Ï„â¿â»Â¹]â‚] also called powers of tau",
            ),
            (
                "ecc_commitment",
                "commit(srs_g1, blob) -> commitment C = âˆ‘ blobáµ¢.srs_g1áµ¢ = âˆ‘ [blobáµ¢.Ï„â±]â‚ = [p(Ï„)]â‚",
            ),
            ("ecc_witness", "w(x, z) = (p(x) - p(z)) / (x-z)"),
            ("ecc_verification", "e([proof]â‚, [Ï„]â‚‚ - [z]â‚‚) = e(C - [y]â‚, [1]â‚‚)"),
            ("ecc_lagrange", "Ï‰ âˆˆ ğ”½r a root of unity of order n, i.e. Ï‰â¿ = 1"),
        ]
    )

    # Group theory refresher
    texts.extend(
        [
            (
                "group_basics",
                "A group is a set of elements with a binary operation called the group law with a neutral element and an inverse.",
            ),
            (
                "field_basics",
                "A field is a set of elements with two group laws, addition and multiplication, with corresponding inverse properties.",
            ),
            ("finite_field", "ğ”½r is a finite-field of prime order r"),
            (
                "cyclic_group",
                "The group can be cyclic, i.e. all elements of the group can be generated by repeatedly applying the group law.",
            ),
            (
                "additive_notation",
                "[a]P to represent P+P+ .... + P, applying the group law a times, i.e. the scalar multiplication.",
            ),
        ]
    )

    # Polynomial commitment scheme
    texts.extend(
        [
            (
                "pcs_prover",
                "Prover computes witness polynomial w(x, z) = (p(x) - p(z)) / (x-z)",
            ),
            (
                "pcs_verifier",
                "Verifier checks proof.(Ï„-z) = p(Ï„) - y using bilinear pairing",
            ),
            (
                "pcs_fiat_shamir",
                "To make the protocol non-interactive, z may be computed via the Fiat-Shamir heuristic.",
            ),
            (
                "pcs_schwartz",
                "According to the Schwartz-zippel Lemma it is cryptographically unlikely that this equation holds",
            ),
            (
                "pcs_srs_g1",
                "srs_g1: [[1]â‚, [Ï„]â‚, [Ï„Â²]â‚, ... [Ï„â¿â»Â¹]â‚] also called powers of tau, with a bounded degree n-1",
            ),
            ("pcs_srs_g2", "srs_g2: [[1]â‚‚, [Ï„]â‚‚]"),
            (
                "pcs_trust_setup",
                "Ï„ and its powers are secrets that no one knows, we only work with [Ï„â±]â‚ and [Ï„]â‚‚ not with Ï„ directly",
            ),
            ("pcs_monomial", "p(x) = blobâ‚€ + blobâ‚ x + blobâ‚‚ xÂ² + ... + blobâ‚™â‚‹â‚ xâ¿â»Â¹"),
        ]
    )

    return texts


def generate_tiktoken_fixtures():
    """Generate fixtures for all tiktoken files."""
    built_in_encodings = {"r50k_base", "p50k_base", "cl100k_base", "o200k_base"}

    for tokenizer_name, filename in TIKTOKEN_FILES.items():
        path = TOKENIZERS_DIR / filename
        if not path.exists():
            print(f"[SKIP] {path} not found")
            continue

        print(f"Processing tiktoken {tokenizer_name}...")

        if tokenizer_name in built_in_encodings:
            encoding = tiktoken.get_encoding(tokenizer_name)
        else:
            # Custom tiktoken file - load from file with pattern
            mergeable_ranks = hf_to_tiktoken.parse_tiktoken_file(str(path))
            encoding = tiktoken.Encoding(
                name=tokenizer_name,
                pat_str=hf_to_tiktoken.KIMI_K25_PATTERN,
                mergeable_ranks=mergeable_ranks,
                special_tokens={},
            )

        fixtures = []
        for name, text in get_test_texts():
            fixture = generate_fixture(text, filename, encoding)
            fixtures.append({"name": name, **fixture})

        output_path = FIXTURES_DIR / f"tiktoken_{tokenizer_name}.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(fixtures, f, ensure_ascii=False, indent=2)
        print(f"  [OK] Generated {len(fixtures)} fixtures to {output_path}")


def generate_tiktoken_from_hf():
    """Generate fixtures for HF tokenizers using tiktoken.Encoding via conversion."""

    for hf_name, hf_filename in HF_FILES.items():
        hf_path = TOKENIZERS_DIR / hf_filename
        if not hf_path.exists():
            print(f"[SKIP] {hf_path} not found")
            continue

        print(f"Processing tiktoken from HF {hf_name}...")
        mergeable_ranks = hf_to_tiktoken.convert_vocab_to_mergeable_ranks(str(hf_path))
        pat_str = hf_to_tiktoken.extract_pattern(str(hf_path))
        special_tokens = hf_to_tiktoken.extract_special_tokens(str(hf_path))

        encoding = tiktoken.Encoding(
            name=hf_name,
            pat_str=pat_str,
            mergeable_ranks=mergeable_ranks,
            special_tokens=special_tokens,
        )

        fixtures = []
        for name, text in get_test_texts():
            fixture = generate_fixture(text, hf_filename, encoding)
            fixtures.append({"name": name, **fixture})

        output_path = FIXTURES_DIR / f"tiktoken_from_hf_{hf_name}.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(fixtures, f, ensure_ascii=False, indent=2)
        print(f"  [OK] Generated {len(fixtures)} fixtures to {output_path}")


def main():
    """Generate all fixtures."""
    print("=" * 70)
    print("Generating tokenizer fixtures via tiktoken")
    print("=" * 70)

    generate_tiktoken_fixtures()
    generate_tiktoken_from_hf()

    print("=" * 70)
    print("Fixture generation complete")
    print("=" * 70)


if __name__ == "__main__":
    main()
